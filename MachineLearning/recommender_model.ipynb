{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(original_dir, subset_dir, num_images_per_class):\n",
    "    if not os.path.exists(subset_dir):\n",
    "        os.makedirs(subset_dir)\n",
    "    \n",
    "    for class_name in os.listdir(original_dir):\n",
    "        class_dir = os.path.join(original_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue  # Skip non-directory files like .DS_Store\n",
    "        subset_class_dir = os.path.join(subset_dir, class_name)\n",
    "        \n",
    "        if not os.path.exists(subset_class_dir):\n",
    "            os.makedirs(subset_class_dir)\n",
    "        \n",
    "        images = os.listdir(class_dir)\n",
    "        selected_images = random.sample(images, min(num_images_per_class, len(images)))\n",
    "        \n",
    "        for image_name in selected_images:\n",
    "            src = os.path.join(class_dir, image_name)\n",
    "            dst = os.path.join(subset_class_dir, image_name)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "# Paths\n",
    "train_dir = \"/Users/Barbara/Downloads/food11/training\"\n",
    "val_dir   = \"/Users/Barbara/Downloads/food11/validation\"\n",
    "test_dir  = \"/Users/Barbara/Downloads/food11/evaluation\"\n",
    "\n",
    "train_subset_dir = \"/Users/Barbara/Downloads/food11/train_subset\"\n",
    "val_subset_dir = \"/Users/Barbara/Downloads/food11/val_subset\"\n",
    "test_subset_dir = \"/Users/Barbara/Downloads/food11/test_subset\"\n",
    "\n",
    "# Create subsets with a limited number of images per class\n",
    "create_subset(train_dir, train_subset_dir, 48)\n",
    "create_subset(val_dir, val_subset_dir, 48)\n",
    "create_subset(test_dir, test_subset_dir, 48)\n",
    "\n",
    "target_size = (224, 224)\n",
    "batch_size = 16\n",
    "\n",
    "# Custom Data Generator\n",
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    def flow_from_directory(self, directory, *args, **kwargs):\n",
    "        generator = super().flow_from_directory(directory, *args, **kwargs)\n",
    "        self.target_size = kwargs.get('target_size', (224, 224))\n",
    "        self.num_classes = generator.num_classes\n",
    "        self.filepaths = generator.filepaths\n",
    "        self.labels = generator.classes\n",
    "        generator._get_batches_of_transformed_samples = self._get_batches_of_transformed_samples\n",
    "        return generator\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.target_size + (3,), dtype=self.dtype)\n",
    "        batch_y = np.zeros((len(index_array), self.num_classes), dtype=self.dtype)\n",
    "        \n",
    "        for i, j in enumerate(index_array):\n",
    "            img_path = self.filepaths[j]\n",
    "            img = safe_load_img(img_path, self.target_size)\n",
    "            if img is not None:\n",
    "                batch_x[i] = img\n",
    "                batch_y[i] = self._get_onehot(self.labels[j])\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def _get_onehot(self, label):\n",
    "        onehot = np.zeros(self.num_classes)\n",
    "        onehot[label] = 1.0\n",
    "        return onehot\n",
    "\n",
    "# Function to safely load images\n",
    "def safe_load_img(path, target_size=(224, 224)):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        img = img.resize(target_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        return img_array\n",
    "    except UnidentifiedImageError:\n",
    "        return None\n",
    "\n",
    "# Data Generators with error handling\n",
    "train_datagen = CustomImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "valid_datagen = CustomImageDataGenerator(rescale=1./255)\n",
    "test_datagen = CustomImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_subset_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    val_subset_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_subset_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Print labels to ensure correct mapping\n",
    "labels = list(test_generator.class_indices.keys())\n",
    "print(\"Number of classes:\", len(labels))\n",
    "print(labels)\n",
    "\n",
    "# Use a smaller model for faster training\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "num_classes = len(labels)  # Ensure num_classes matches the number of classes in the data\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Adding dropout for regularization\n",
    "out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "num_epochs = 5  # Reduced number of epochs for faster training\n",
    "STEP_SIZE_TRAIN = min(len(train_generator), train_generator.n // train_generator.batch_size)\n",
    "STEP_SIZE_VALID = min(len(valid_generator), valid_generator.n // valid_generator.batch_size)\n",
    "STEP_SIZE_TEST = min(len(test_generator), test_generator.n // test_generator.batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('/Users/Barbara/Desktop/Ironhack/Final_Project/food_recognition_model4.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, acc = model.evaluate(test_generator, steps=STEP_SIZE_TEST)\n",
    "print(f\"Test accuracy: {acc:.3f}\\nTest Loss: {loss:.3f}\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "preds = model.predict(test_generator)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "y_actual = test_generator.classes\n",
    "cm = confusion_matrix(y_actual, y_pred)\n",
    "print(cm)\n",
    "print(classification_report(y_actual, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9866 images belonging to 11 classes.\n",
      "Found 3430 images belonging to 11 classes.\n",
      "Found 3347 images belonging to 11 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 16:21:21.735059: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-07-10 16:21:21.735081: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-07-10 16:21:21.735089: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-07-10 16:21:21.735282: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-10 16:21:21.735295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">19,267,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m19,267,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m1,419\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,269,131</span> (73.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,269,131\u001b[0m (73.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,269,131</span> (73.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,269,131\u001b[0m (73.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 16:21:43.898043: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x37df89550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x37df89550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Batch 10: Loss = 38459.9570, Accuracy = 0.1312\n",
      "Batch 20: Loss = 32780.9414, Accuracy = 0.1328\n",
      "Batch 30: Loss = 25212.1055, Accuracy = 0.1250\n",
      "Batch 40: Loss = 20246.5820, Accuracy = 0.1273\n",
      "Batch 50: Loss = 16898.8438, Accuracy = 0.1256\n",
      "Batch 60: Loss = 14485.8096, Accuracy = 0.1297\n",
      "Batch 70: Loss = 12734.6758, Accuracy = 0.1299\n",
      "Batch 80: Loss = 11341.3545, Accuracy = 0.1301\n",
      "Batch 90: Loss = 10213.5107, Accuracy = 0.1288\n",
      "Batch 100: Loss = 9319.8447, Accuracy = 0.1291\n",
      "Batch 110: Loss = 8579.9160, Accuracy = 0.1261\n",
      "Batch 120: Loss = 7948.5952, Accuracy = 0.1268\n",
      "Batch 130: Loss = 7416.0024, Accuracy = 0.1238\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     80\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_generator:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_batches \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_generator):\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Utilities/anaconda3/envs/tensorflow_macos/lib/python3.9/site-packages/keras/src/legacy/preprocessing/image.py:112\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/Utilities/anaconda3/envs/tensorflow_macos/lib/python3.9/site-packages/keras/src/legacy/preprocessing/image.py:313\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    311\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[0;32m--> 313\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     x \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mimg_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Utilities/anaconda3/envs/tensorflow_macos/lib/python3.9/site-packages/keras/src/utils/image_utils.py:236\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be path-like or io.BytesIO, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Custom ImageDataGenerator class\n",
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    def flow_from_directory(self, directory, *args, **kwargs):\n",
    "        generator = super().flow_from_directory(directory, *args, **kwargs)\n",
    "        self.target_size = kwargs.get('target_size', (224, 224))\n",
    "        self.num_classes = generator.num_classes\n",
    "        self.filepaths = generator.filepaths\n",
    "        self.labels = generator.classes\n",
    "        return generator\n",
    "\n",
    "# Function to safely load images\n",
    "def safe_load_img(path, target_size=(224, 224)):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        img = img.resize(target_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        return img_array / 255.0  # Normalize here\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Data Generators\n",
    "train_datagen = CustomImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "valid_datagen = CustomImageDataGenerator()\n",
    "test_datagen = CustomImageDataGenerator()\n",
    "\n",
    "# Directories for data\n",
    "train_dir = \"/Users/Barbara/Desktop/Ironhack/Final_Project/food11/training\"\n",
    "valid_dir   = \"/Users/Barbara/Desktop/Ironhack/Final_Project/food11/validation\"\n",
    "test_dir  = \"/Users/Barbara/Desktop/Ironhack/Final_Project/food11/evaluation\"\n",
    "\n",
    "# Generators\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Get the number of classes from the train generator\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "# Use a simpler model for testing\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Custom training loop\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_generator:\n",
    "        if train_batches >= len(train_generator):\n",
    "            break\n",
    "        \n",
    "        train_result = model.train_on_batch(x_batch, y_batch)\n",
    "        train_loss += train_result[0]\n",
    "        train_accuracy += train_result[1]\n",
    "        train_batches += 1\n",
    "        \n",
    "        if train_batches % 10 == 0:\n",
    "            print(f\"Batch {train_batches}: Loss = {train_result[0]:.4f}, Accuracy = {train_result[1]:.4f}\")\n",
    "    \n",
    "    train_loss /= train_batches\n",
    "    train_accuracy /= train_batches\n",
    "    print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in valid_generator:\n",
    "        if val_batches >= len(valid_generator):\n",
    "            break\n",
    "        \n",
    "        val_result = model.test_on_batch(x_batch, y_batch)\n",
    "        val_loss += val_result[0]\n",
    "        val_accuracy += val_result[1]\n",
    "        val_batches += 1\n",
    "    \n",
    "    val_loss /= val_batches\n",
    "    val_accuracy /= val_batches\n",
    "    print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('/Users/Barbara/Desktop/Ironhack/Final_Project/food_recognition_model2.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "test_batches = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for x_batch, y_batch in test_generator:\n",
    "    if test_batches >= len(test_generator):\n",
    "        break\n",
    "    \n",
    "    test_result = model.test_on_batch(x_batch, y_batch)\n",
    "    test_loss += test_result[0]\n",
    "    test_accuracy += test_result[1]\n",
    "    \n",
    "    predictions = model.predict_on_batch(x_batch)\n",
    "    all_predictions.extend(np.argmax(predictions, axis=1))\n",
    "    all_true_labels.extend(np.argmax(y_batch, axis=1))\n",
    "    \n",
    "    test_batches += 1\n",
    "\n",
    "test_loss /= test_batches\n",
    "test_accuracy /= test_batches\n",
    "print(f\"Test - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
